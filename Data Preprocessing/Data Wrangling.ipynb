{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data path\n",
    "path0 = 'Data/'\n",
    "filename = 'reviews_Amazon_Instant_Video_5.json.gz'\n",
    "path = path0 + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### function to extract and parse the data\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>A11N155CW1UV02</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>AdrianaM</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I had big expectations because I love English ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>A little bit boring for me</td>\n",
       "      <td>1399075200</td>\n",
       "      <td>05 3, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>A3BC8O2KCL29V2</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>Carol T</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I highly recommend this series. It is a must f...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent Grown Up TV</td>\n",
       "      <td>1346630400</td>\n",
       "      <td>09 3, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>A60D5HQFOTSOM</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>Daniel Cooper \"dancoopermedia\"</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>This one is a real snoozer. Don't believe anyt...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Way too boring for me</td>\n",
       "      <td>1381881600</td>\n",
       "      <td>10 16, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A1RJPIGRSNX4PW</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>J. Kaplan \"JJ\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Mysteries are interesting.  The tension betwee...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Robson Green is mesmerizing</td>\n",
       "      <td>1383091200</td>\n",
       "      <td>10 30, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>A16XRPF40679KG</td>\n",
       "      <td>B000H00VBQ</td>\n",
       "      <td>Michael Dobey</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>This show always is excellent, as far as briti...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Robson green and great writing</td>\n",
       "      <td>1234310400</td>\n",
       "      <td>02 11, 2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                    reviewerName helpful  \\\n",
       "0  A11N155CW1UV02  B000H00VBQ                        AdrianaM  [0, 0]   \n",
       "1  A3BC8O2KCL29V2  B000H00VBQ                         Carol T  [0, 0]   \n",
       "2   A60D5HQFOTSOM  B000H00VBQ  Daniel Cooper \"dancoopermedia\"  [0, 1]   \n",
       "3  A1RJPIGRSNX4PW  B000H00VBQ                  J. Kaplan \"JJ\"  [0, 0]   \n",
       "4  A16XRPF40679KG  B000H00VBQ                   Michael Dobey  [1, 1]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  I had big expectations because I love English ...      2.0   \n",
       "1  I highly recommend this series. It is a must f...      5.0   \n",
       "2  This one is a real snoozer. Don't believe anyt...      1.0   \n",
       "3  Mysteries are interesting.  The tension betwee...      4.0   \n",
       "4  This show always is excellent, as far as briti...      5.0   \n",
       "\n",
       "                          summary  unixReviewTime   reviewTime  \n",
       "0      A little bit boring for me      1399075200   05 3, 2014  \n",
       "1           Excellent Grown Up TV      1346630400   09 3, 2012  \n",
       "2           Way too boring for me      1381881600  10 16, 2013  \n",
       "3     Robson Green is mesmerizing      1383091200  10 30, 2013  \n",
       "4  Robson green and great writing      1234310400  02 11, 2009  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['reviewText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enjoyed some of the comedians, it was a joy to laugh after losing my father whom I was a caregiver for'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Cleaning Special Characters and Removing Punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preprocesssing that will be common to all the text classification methods you will see. \n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, Dataset, Example\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameDataset(Dataset):\n",
    "    \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
    "    def __init__(self, examples, fields, filter_pred=None):\n",
    "        \"\"\"\n",
    "        Create a dataset from a pandas dataframe of examples and Fields\n",
    "        Arguments:\n",
    "         examples pd.DataFrame: DataFrame of examples\n",
    "         fields {str: Field}: The Fields to use in this tuple. The\n",
    "             string is a field name, and the Field is the associated field.\n",
    "         filter_pred (callable or None): use only exanples for which\n",
    "             filter_pred(example) is true, or use all examples if None.\n",
    "             Default is None\n",
    "        \"\"\"\n",
    "        self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
    "        if filter_pred is not None:\n",
    "            self.examples = filter(filter_pred, self.examples)\n",
    "        self.fields = dict(fields)\n",
    "        # Unpack field tuples\n",
    "        for n, f in list(self.fields.items()):\n",
    "             if isinstance(n, tuple):\n",
    "                    self.fields.update(zip(n, f))\n",
    "                    del self.fields[n]\n",
    "\n",
    "class SeriesExample(Example):\n",
    "    \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
    "    @classmethod\n",
    "    def fromSeries(cls, data, fields):\n",
    "        return cls.fromdict(data.to_dict(), fields)\n",
    "\n",
    "    @classmethod\n",
    "    def fromdict(cls, data, fields):\n",
    "        ex = cls()\n",
    "\n",
    "        for key, field in fields.items():\n",
    "            if key not in data:\n",
    "                raise ValueError(\"Specified key {} was not found in \"\n",
    "                         \"the input data\".format(key))\n",
    "            if field is not None:\n",
    "                setattr(ex, key, field.preprocess(data[key]))\n",
    "            else:\n",
    "                setattr(ex, key, data[key])\n",
    "        return ex\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.reviewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import LabelField, Field\n",
    "TEXT = Field(tokenize='spacy')\n",
    "LABEL = LabelField(dtype=torch.float)\n",
    "ds = DataFrameDataset(df, {'reviewText': TEXT, 'overall': LABEL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = ds.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 91604), ('the', 80780), (',', 69030), ('and', 48842), ('a', 41598), ('to', 41079), ('of', 37531), ('I', 34246), ('is', 32192), (' ', 30126), ('it', 23554), ('in', 21008), ('that', 18869), ('this', 17716), (\"'s\", 14395), ('for', 13264), ('with', 13043), ('The', 12307), ('show', 12038), ('-', 11934)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '.', 'the', ',', 'and', 'a', 'to', 'of', 'I']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {5.0: 0, 4.0: 1, 3.0: 2, 2.0: 3, 1.0: 4})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "from torchtext import data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.reviewText).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.overall)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.overall)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.reviewText).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.overall)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.overall)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'SeriesExample' and 'SeriesExample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-18c4bbde3ebb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-156-5d4edf147647>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, criterion)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreviewText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;31m# fast-forward if loaded from state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36minit_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state_this_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restored_from_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36mcreate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             self.batches = batch(self.data(), self.batch_size,\n\u001b[0m\u001b[1;32m    246\u001b[0m                                  self.batch_size_fn)\n\u001b[1;32m    247\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m\"\"\"Return the examples in the dataset in order, sorted, or shuffled.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_shuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'SeriesExample' and 'SeriesExample'"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_test, y_train, y_text = train_test_split(X, y, stratify=y, random_state =42, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=3,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(text_train)\n",
    "X_test = vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29700, 22091)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '007', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '100th', '101', '102', '104', '105', '108', '1080p', '10pm', '10th', '11', '110', '112', '116', '117', '11th', '12', '12th', '13', '13th', '14', '143', '14th', '15', '150', '15th', '16', '1600', '165', '169', '16mm', '16th', '17', '1776', '17th', '18', '180', '1800', '1800s', '1844', '1860', '1864', '1865', '1890', '18th', '19', '1900', '1900s', '1906', '1908', '1914', '1920', '1920s', '1930', '1930s', '1931', '1940', '1940s', '1945', '1950', '1950s', '1956', '1959', '1960', '1960s', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1970s', '1971', '1972', '1973', '1975', '1976', '1977', '1978', '1979', '1980', '1980s', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1990s', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '19th', '1hr', '1st', '20', '200', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '201', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2077', '20s', '20th', '21', '21st', '22', '22nd', '23', '24', '25', '250', '256', '25th', '26', '27', '28', '28th', '29', '2am', '2d', '2nd', '30', '300', '3000', '30s', '31', '32', '33', '34', '35', '36', '360', '37', '38', '39', '3am', '3d', '3rd', '3x', '40', '400', '407', '40s', '41', '42', '43', '44', '4400', '45', '46', '47', '48', '49', '4th', '4x', '4yr', '50', '500', '5000', '50s', '50th', '51', '52', '53', '54', '55', '56', '57', '58', '59', '5th', '60', '600', '60s', '60th', '61', '62', '63', '65', '666', '67', '69', '6th', '70', '700', '70s', '71', '74', '75', '76', '77', '78', '79', '7th', '80', '800', '80s', '82', '8203', '8211', '8212', '8216', '8217', '8220', '8221', '8230', '8243', '83', '84', '85', '86', '87', '88', '89', '8th', '90', '900', '90210', '90s', '91', '911', '92', '93', '94', '95', '96', '97', '98', '99', '9pm', '9th', 'aa', 'aacute', 'aang', 'aaron', 'ab', 'abandon', 'abandoned', 'abandoning', 'abbey', 'abbie', 'abbot', 'abbreviated', 'abby', 'abc', 'abcs', 'abducted', 'abducting', 'abduction', 'abductions', 'abducts', 'abe', 'abed', 'abel', 'aberrant', 'abfab', 'abiding', 'abigail', 'abilities', 'ability', 'abit', 'abject', 'ablaze', 'able', 'ably', 'abnormal', 'abnormals', 'aboard', 'abode', 'abominable', 'abomination', 'aboriginal', 'abortion', 'abound', 'abounds', 'about', 'above', 'abraham', 'abrams', 'abrasive', 'abroad', 'abrupt', 'abruptly', 'absaroka', 'absence', 'absent', 'absolute', 'absolutely', 'absolutly', 'absorb', 'absorbed', 'absorbing', 'absorption', 'abstract', 'absurd', 'absurdist', 'absurdities', 'absurdity', 'absurdly', 'abundance', 'abundant', 'abundantly', 'abuse', 'abused', 'abuser', 'abusers', 'abuses', 'abusing', 'abusive', 'abysmal', 'abyss', 'academic', 'academy', 'accelerated', 'accent', 'accented', 'accents', 'accentuate', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepting', 'accepts', 'access', 'accessible', 'accessing', 'accessory', 'accident', 'accidental', 'accidentally', 'accidently', 'accidents', 'acclaim', 'acclaimed', 'accolades', 'accompanied', 'accompanies', 'accompany', 'accompanying', 'accomplice', 'accomplices', 'accomplish', 'accomplished', 'accomplishes', 'accomplishing', 'accomplishment', 'accomplishments', 'according', 'accordingly', 'account', 'accountant', 'accounted', 'accounting', 'accounts', 'accoutrements', 'accuracy', 'accurate', 'accurately', 'accusation', 'accusations', 'accused', 'accuses', 'accusing', 'accustomed', 'ace', 'acerbic', 'aces', 'ache', 'achieve', 'achieved', 'achievement', 'achievements', 'achieves', 'achieving', 'aching', 'achingly', 'acid', 'acker', 'ackles', 'acknowledge', 'acknowledged', 'acknowledging', 'acolytes', 'acorn', 'acquaintance', 'acquaintances', 'acquainted', 'acquire', 'acquired', 'acquiring', 'acquisition', 'acquits', 'acquitted', 'acres', 'acrobatic', 'acrobatics', 'across', 'act', 'acted', 'acting', 'action', 'actioner', 'actions', 'activate', 'activated', 'activates', 'active', 'actively', 'actives', 'activism', 'activist', 'activists', 'activities', 'activity', 'actor', 'actors', 'actress', 'actresses', 'acts', 'actual', 'actuality', 'actually', 'acuity', 'acumen', 'ad', 'ada', 'adage', 'adam', 'adama', 'adams', 'adapt', 'adaptation', 'adaptations', 'adapted', 'adapting', 'adaption', 'adaptions', 'adapts', 'add', 'added', 'addendum', 'addict', 'addicted', 'addicting', 'addiction', 'addictions', 'addictive', 'addictively', 'addicts', 'adding', 'addison', 'addition', 'additional', 'additionally', 'additions', 'additive', 'addled', 'address', 'addressed', 'addresses', 'addressing', 'adds', 'addy', 'adelaide', 'adelle', 'adept', 'adequate', 'adequately', 'adewale', 'adhd', 'adhere', 'adhered', 'adherence', 'adheres', 'adib', 'adjacent', 'adjective', 'adjectives', 'adjust', 'adjusted', 'adjusting', 'adjustment', 'adjustments', 'adkins', 'adler', 'adlon', 'administration', 'administrative', 'administrator', 'admirable', 'admirably', 'admiral', 'admiration', 'admire', 'admired', 'admirer', 'admires', 'admiring', 'admission', 'admit', 'admits', 'admitted', 'admittedly', 'admitting', 'adolescence', 'adolescent', 'adolescents', 'adopt', 'adopted', 'adoption', 'adoptive', 'adopts', 'adorable', 'adorably', 'adoration', 'adore', 'adored', 'adores', 'adoring', 'adrenaline', 'adrian', 'adrien', 'adrienne', 'adroit', 'ads', 'adult', 'adulterous', 'adultery', 'adulthood', 'adults', 'advance', 'advanced', 'advancement', 'advances', 'advancing', 'advantage', 'advantages', 'advent', 'adventure', 'adventurer', 'adventurers', 'adventures', 'adventurous', 'adversarial', 'adversaries', 'adversary', 'adverse', 'adversity', 'advertise', 'advertised', 'advertisement', 'advertisements', 'advertisers', 'advertising', 'advertizing', 'advice', 'advise', 'advised', 'adviser', 'advises', 'advising', 'advisor', 'advisors', 'advocate', 'advocates', 'aerial', 'aesthetic', 'aesthetically', 'aesthetics', 'afar', 'affable', 'affair', 'affairs', 'affect', 'affected', 'affecting', 'affection', 'affectionate', 'affectionately', 'affections', 'affects', 'affiliated', 'affinity', 'affirmation', 'affirming', 'affleck', 'afflicted', 'affliction', 'affluent', 'afford', 'affordable', 'afghanistan', 'aficionado', 'aficionados', 'afloat', 'afoot', 'aforementioned', 'afoul', 'afraid', 'africa', 'african', 'after', 'afterall', 'afterlife', 'aftermath', 'afternoon', 'aftershock', 'afterthought', 'afterward', 'afterwards', 'again', 'against', 'agata', 'agatha', 'agathon', 'agbaje', 'age', 'aged', 'ageless', 'agencies', 'agency', 'agenda', 'agendas', 'agent', 'agents', 'agers', 'ages', 'agey', 'aggravated', 'aggravating', 'aggression', 'aggressive', 'aggressively', 'aging', 'agnes', 'agnostic', 'ago', 'agonizing', 'agony', 'agoraphobia', 'agoraphobic', 'agrave', 'agree', 'agreed', 'agreeing', 'agreement', 'agreements', 'agrees', 'agricultural', 'agron', 'ah', 'aha', 'ahead', 'ahem', 'ahh', 'ahmed', 'ahold', 'ahs', 'ai', 'aid', 'aidan', 'aide', 'aided', 'aiden', 'aides', 'aiding', 'aids', 'ailing', 'aim', 'aimed', 'aimee', 'aiming', 'aimless', 'aimlessly', 'aims', 'ain', 'air', 'airbender', 'airbending', 'airborne', 'aircraft', 'aired', 'airhead', 'airing', 'airings', 'airlock', 'airplane', 'airplanes', 'airport', 'airports', 'airs', 'airtime', 'airwaves', 'aisha', 'aisle', 'aja', 'aka', 'akin', 'akinnuoye', 'akira', 'akron', 'al', 'ala', 'alabama', 'alaina', 'alan', 'alana', 'alanis', 'alarm', 'alarming', 'alarmingly', 'alas', 'alaska', 'alaskan', 'albeit', 'albert', 'alberta', 'albino', 'album', 'albuquerque', 'alcatraz', 'alcohol', 'alcoholic', 'alcoholics', 'alcoholism', 'alda', 'alderman', 'aldis', 'alec', 'aleck', 'alejandro', 'alert', 'alerted', 'alerts', 'alex', 'alexander', 'alexandra', 'alexandre', 'alexia', 'alexis', 'alf', 'alfred', 'ali', 'alias', 'alice', 'alicia', 'alicja', 'alien', 'alienate', 'alienated', 'alienates', 'alienating', 'alienation', 'aliens', 'align', 'aligned', 'alike', 'alison', 'alistair', 'alittle', 'alive', 'all', 'allam', 'allan', 'alleged', 'allegedly', 'allegiance', 'allegiances', 'allegorical', 'allegory', 'allen', 'allergic', 'alleviate', 'alley', 'alliance', 'alliances', 'allie', 'allies', 'alligator', 'alligators', 'allison', 'allot', 'allotted', 'allow', 'allowances', 'allowed', 'allowing', 'allows', 'alluded', 'alludes', 'allure', 'alluring', 'allusion', 'allusions', 'ally', 'alma', 'almeida', 'almighty', 'almost', 'alone', 'along', 'alongside', 'aloof', 'alot', 'aloud', 'alpha', 'alphabet', 'alphas', 'already', 'alright', 'also', 'altar', 'alter', 'alterations', 'altered', 'altering', 'alternate', 'alternately', 'alternates', 'alternating', 'alternative', 'alternatives', 'alters', 'altho', 'although', 'altitude', 'altman', 'altogether', 'alton', 'altruistic', 'alum', 'aluminum', 'alumni', 'alums', 'alway', 'always', 'alyson', 'alyssa', 'alzheimer', 'am', 'amalgam', 'amanda', 'amassed', 'amateur', 'amateurish', 'amateurs', 'amaze', 'amazed', 'amazement', 'amazes', 'amazing', 'amazingly', 'amazon', 'amazons', 'ambassador', 'amber', 'ambiance', 'ambience', 'ambient', 'ambiguity', 'ambiguous', 'ambiguously', 'ambition', 'ambitions', 'ambitious', 'ambivalence', 'ambivalent', 'ambrose', 'ambulance', 'ambush', 'ambushed', 'amc', 'amelia', 'amell', 'amen', 'amendment', 'amends', 'america', 'american', 'americana', 'americanized', 'americans', 'americas', 'ames', 'amiable', 'amid', 'amidst', 'amish', 'amiss', 'amityville', 'ammo', 'ammunition', 'amnesia', 'amok', 'among', 'amongst', 'amoral', 'amores', 'amorphous', 'amount', 'amounted', 'amounts', 'amp', 'amped', 'ample', 'amplified', 'amps', 'amuck', 'amuse', 'amused', 'amusement', 'amusing', 'amusingly', 'amy', 'an', 'ana', 'anachronisms', 'anachronistic', 'analog', 'analogies', 'analogy', 'analysis', 'analyst', 'analysts', 'analytical', 'analyze', 'analyzing', 'anamaria', 'anamorphic', 'anarchic', 'anarchistic', 'anarchy', 'anastasia', 'anatomy', 'ancestors', 'ancestry', 'anchor', 'anchored', 'anchorman', 'anchors', 'ancient', 'ancients', 'ancillary', 'and', 'anda', 'anders', 'anderson', 'andie', 'ando', 'andr', 'andre', 'andrea', 'andrei', 'andrew', 'android', 'andromeda', 'andthe', 'andy', 'anecdotal', 'anecdotes', 'anesthesia', 'anew', 'ang', 'angarano', 'angel', 'angela', 'angeles', 'angelic', 'angelica', 'angelina', 'angels', 'anger', 'angie', 'angle', 'angles', 'anglo', 'anglophile', 'angry', 'angst', 'anguish', 'anguished', 'angus', 'anika', 'animal', 'animalistic', 'animals', 'animate', 'animated', 'animatics', 'animation', 'animations', 'animators', 'anime', 'animosity', 'aniston', 'anita', 'anjelica', 'ankle', 'ann', 'anna', 'annabeth', 'annals', 'anne', 'annebots', 'annie', 'annihilated', 'annihilation', 'annis', 'anniversary', 'announce', 'announced', 'announcement', 'announcements', 'announcer', 'announces', 'announcing', 'annoy', 'annoyance', 'annoyed', 'annoying', 'annoyingly', 'annoys', 'annual', 'anomalies', 'anomaly', 'anonymous', 'another', 'ans', 'ansari', 'anson', 'answer', 'answered', 'answering', 'answers', 'ant', 'antagonism', 'antagonist', 'antagonistic', 'antagonists', 'ante', 'antenna', 'anthologies', 'anthology', 'anthony', 'anthropological', 'anthropologist', 'anthropology', 'anti', 'antibiotics', 'antic', 'antichrist', 'anticipate', 'anticipated', 'anticipating', 'anticipation', 'anticlimactic', 'antics', 'antique', 'antiques', 'antisocial', 'antithesis', 'antiviral', 'antoinette', 'anton', 'antonio', 'antony', 'antsy', 'antwon']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[:1111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vamp', 'vampire', 'vampires', 'vampiric', 'vampirism', 'vamps', 'van', 'vancamp', 'vance', 'vancouver', 'vanderbilt', 'vandervoort', 'vanessa', 'vanilla', 'vanish', 'vanished', 'vanishes', 'vanishing', 'vanity', 'vantage', 'vapid', 'variable', 'variant', 'variation', 'variations', 'varied', 'varies', 'varieties', 'variety', 'various', 'varma', 'vary', 'varying', 'vast', 'vastly', 'vaughn', 'vault', 'vcr', 've', 'vector', 'vectors', 'veep', 'veer', 'veered', 'veering', 'veers', 'vega', 'vegan', 'veganism', 'vegans', 'vegas', 'vegetable', 'vegetables', 'vegetarian', 'vehicle', 'vehicles', 'veil', 'veiled', 'vein', 'veins', 'velez', 'velvet', 'venal', 'vendetta', 'veneer', 'venerable', 'vengeance', 'vengeful', 'venice', 'venom', 'ventimiglia', 'ventriloquist', 'venture', 'ventured', 'ventures', 'venturing', 'venue', 'venues', 'venus', 'vera', 'verbal', 'verbally', 'verbatim', 'verdict', 'verdictread', 'vergara', 'verge', 'verges', 'verging', 'veridian', 'verified', 'verify', 'verisimilitude', 'veritable', 'vermont', 'vernon', 'veronica', 'versa', 'versace', 'versailles', 'versatile', 'versatility', 'verse', 'versed', 'verses', 'version', 'versions', 'versus', 'vertical', 'vertigo', 'verve', 'very', 'vessel', 'vessels', 'vested', 'vests', 'vesuvius', 'vet', 'veteran', 'veterans', 'veterinarian', 'vets', 'vey', 'vfx', 'vhs', 'vi', 'via', 'viable', 'vibe', 'vibes', 'vibrancy', 'vibrant', 'vic', 'vicar', 'vicarious', 'vicariously', 'vice', 'vicinity', 'vicious', 'viciously', 'vick', 'vicky', 'victim', 'victimize', 'victimized', 'victims', 'victor', 'victoria', 'victorian', 'victories', 'victorious', 'victory', 'video', 'videodrome', 'videogame', 'videography', 'videos', 'videotape', 'videotapes', 'videotaping', 'vie', 'vienna', 'viet', 'vietnam', 'view', 'viewable', 'viewed', 'viewer', 'viewers', 'viewership', 'viewing', 'viewings', 'viewpoint', 'viewpoints', 'views', 'vigilant', 'vigilante', 'vigilantism', 'vignettes', 'vigor', 'viii', 'viking', 'vikings', 'vile', 'village', 'villagers', 'villages', 'villain', 'villainous', 'villains', 'villainy', 'villan', 'villian', 'villians', 'vince', 'vincent', 'vinci', 'vindictive', 'vinessa', 'vinnie', 'vintage', 'violated', 'violating', 'violation', 'violence', 'violent', 'violently', 'violet', 'violin', 'vip', 'viral', 'virgin', 'virginia', 'virginity', 'virgins', 'virilus', 'virologist', 'virtual', 'virtually', 'virtue', 'virtues', 'virtuoso', 'virus', 'viruses', 'vis', 'visa', 'visceral', 'visible', 'vision', 'visionary', 'visions', 'visit', 'visitation', 'visited', 'visiting', 'visitor', 'visitors', 'visits', 'vistas', 'visual', 'visually', 'visuals', 'vital', 'vithaya', 'viva', 'vivacious', 'vivian', 'vivid', 'vividly', 'vlad', 'vm', 'vocabulary', 'vocal', 'vocally', 'vocals', 'vod', 'vodka', 'vogel', 'vogue', 'voice', 'voiced', 'voiceover', 'voiceovers', 'voices', 'void', 'voila', 'vol', 'volatile', 'volatility', 'volcanic', 'volcano', 'volleyball', 'volm', 'volume', 'volumes', 'voluntarily', 'volunteer', 'volunteered', 'volunteers', 'vomit', 'vomited', 'vomiting', 'von', 'voodoo', 'vorenus', 'vorian', 'vote', 'voted', 'voters', 'votes', 'voting', 'vow', 'vows', 'voyage', 'voyager', 'voyagers', 'vp', 'vredal', 'vs', 'vu', 'vudu', 'vulcan', 'vulgar', 'vulgarity', 'vulnerability', 'vulnerable', 'vyctoryab', 'vying', 'wa', 'wackiness', 'wackness', 'wacko', 'wacky', 'wad', 'wade', 'waffling', 'wage', 'waging', 'wagner', 'wagon', 'wahlberg', 'waif', 'wain', 'waist', 'wait', 'waite', 'waited', 'waiting', 'waitress', 'waits', 'wake', 'wakefield', 'wakes', 'waking', 'wal', 'wales', 'walk', 'walked', 'walker', 'walkers', 'walkie', 'walking', 'walks', 'wall', 'wallace', 'wallander', 'walled', 'wallender', 'waller', 'wallet', 'wallow', 'wallowing', 'wallows', 'wallpaper', 'walls', 'wally', 'walmart', 'walsh', 'walt', 'walter', 'walters', 'walton', 'waltons', 'waltz', 'wan', 'wanda', 'wander', 'wandered', 'wandering', 'wanders', 'wane', 'waned', 'waning', 'wanna', 'wannabe', 'wannabes', 'want', 'wanted', 'wanting', 'wants', 'war', 'warcraft', 'ward', 'warden', 'wardrobe', 'wardrobes', 'warehouse', 'wares', 'warfare', 'warlord', 'warlords', 'warm', 'warmed', 'warming', 'warmly', 'warmth', 'warn', 'warned', 'warner', 'warning', 'warnings', 'warns', 'warp', 'warped', 'warping', 'warrant', 'warranted', 'warrants', 'warren', 'warrick', 'warring', 'warrior', 'warriors', 'wars', 'warsaw', 'wartime', 'warts', 'wary', 'was', 'wasa', 'wash', 'washed', 'washing', 'washington', 'washy', 'wasn', 'wasnt', 'wasp', 'waste', 'wasted', 'wasteland', 'waster', 'wastes', 'wasting', 'watch', 'watchable', 'watched', 'watcher', 'watchers', 'watches', 'watchin', 'watching', 'watchlist', 'watchmen', 'watcing', 'water', 'watered', 'watering', 'waters', 'watling', 'watson', 'watts', 'waugh', 'wave', 'wavelength', 'waver', 'wavers', 'waves', 'waving', 'wax', 'way', 'wayne', 'ways', 'wayside', 'waythe', 'wayward', 'wayyyyy', 'wb', 'wd', 'we', 'weak', 'weaken', 'weakened', 'weakening', 'weaker', 'weakest', 'weakling', 'weakly', 'weakness', 'weaknesses', 'wealth', 'wealthiest', 'wealthy', 'wean', 'weapon', 'weaponry', 'weapons', 'wear', 'wearing', 'wears', 'weary', 'weasel', 'weather', 'weathered', 'weatherly', 'weave', 'weaved', 'weaver', 'weaves', 'weaving', 'web', 'webcam', 'weber', 'webisodes', 'webs', 'website', 'websites', 'webster', 'wed', 'wedding', 'weddings', 'wedlock', 'wednesday', 'wee', 'weed', 'weeds', 'week', 'weekend', 'weekends', 'weekly', 'weeks', 'weep', 'weeping', 'weepy', 'weevil', 'weigh', 'weighed', 'weighing', 'weighs', 'weight', 'weiner', 'weinstein', 'weir', 'weird', 'weirder', 'weirdest', 'weirdly', 'weirdness', 'weirdo', 'weisz', 'weixler', 'welch', 'welcome', 'welcomed', 'welcomes', 'welcoming', 'welfare', 'well', 'weller', 'welliver', 'wells', 'welsh', 'wen', 'wendi', 'wendy', 'went', 'wentworth', 'were', 'weren', 'werewolf', 'werewolfs', 'werewolves', 'wern', 'werner', 'wersching', 'wes', 'wesen', 'wesley', 'wessen', 'west', 'westen', 'western', 'westerns', 'weston', 'westward', 'wet', 'wether', 'whack', 'whacky', 'whale', 'whales', 'what', 'whately', 'whatever', 'whatnot', 'whats', 'whatsoever', 'wheat', 'wheatley', 'wheaton', 'whedon', 'whedons', 'wheel', 'wheelchair', 'wheeler', 'wheelhouse', 'wheeling', 'wheels', 'when', 'whenever', 'where', 'whereabouts', 'whereas', 'wherefores', 'wherein', 'wherever', 'whet', 'whether', 'whew', 'which', 'whichever', 'whiff', 'while', 'whilst', 'whim', 'whims', 'whimsical', 'whimsy', 'whine', 'whines', 'whiney', 'whining', 'whiny', 'whip', 'whiplash', 'whipping', 'whips', 'whirl', 'whiskey', 'whisper', 'whispered', 'whisperer', 'whispering', 'whispers', 'whistler', 'whistles', 'whit', 'whitaker', 'white', 'whitechapel', 'whites', 'whitlock', 'whitman', 'whitney', 'whiz', 'who', 'whoa', 'whoda', 'whodunit', 'whodunnit', 'whoever', 'whole', 'wholeheartedly', 'wholes', 'wholesome', 'wholly', 'whom', 'whomever', 'whopping', 'whore', 'whores', 'whoring', 'whos', 'whose', 'whovian', 'whovians', 'why', 'whys', 'wi', 'wicked', 'wickedly', 'wicker', 'wide', 'widely', 'widening', 'wider', 'widescreen', 'widespread', 'widow', 'widowed', 'widower', 'widows', 'wiegel', 'wield', 'wielding', 'wields', 'wiener', 'wierd', 'wierdos', 'wife', 'wig', 'wiggin', 'wigs', 'wii', 'wiig', 'wiki', 'wikileaks', 'wikipedia', 'wil', 'wilcox', 'wild', 'wilde', 'wilder', 'wilderness', 'wildest', 'wildhorn', 'wildlife', 'wildly', 'wilds', 'wiley', 'wilkinson', 'will', 'willa', 'willard', 'willed', 'willem', 'willful', 'william', 'williams', 'williamson', 'willie', 'willing', 'willingly', 'willingness', 'willis', 'willow', 'willowbrook', 'willy', 'wilson', 'wily', 'wimp', 'wimpy', 'win', 'wince', 'winchester', 'winchesters', 'wind', 'winded', 'winding', 'window', 'windows', 'winds', 'windshield', 'windy', 'wine', 'winfrey', 'wing', 'wingard', 'wings', 'wink', 'winkle', 'winkler', 'winner', 'winners', 'winnick', 'winning', 'winona', 'wins', 'winston', 'winstone', 'winter', 'winters', 'wipe', 'wiped', 'wiping', 'wire', 'wired', 'wiredweird', 'wires', 'wisdom', 'wise', 'wisecracking', 'wisely', 'wiser', 'wish', 'wished', 'wishes', 'wishful', 'wishing', 'wishy', 'wit', 'witch', 'witchcraft', 'witches', 'witchy', 'with', 'withdrawal', 'withdrawals', 'withdrawn', 'wither', 'within', 'without', 'witness', 'witnessed', 'witnesses', 'witnessing', 'wits', 'witsec', 'witt', 'witted', 'wittgenstein', 'wittiest', 'wittiness', 'witty', 'witwer', 'wives', 'wix', 'wizard', 'wizards', 'wo', 'wobbly', 'woe', 'woeful', 'woefully', 'woes', 'woke', 'wolf', 'wolfman', 'wolowitz', 'wolsey', 'wolverine', 'wolves', 'woman', 'womanizer', 'womanizing', 'womb', 'women', 'womens', 'won', 'wonder', 'wondered', 'wonderful', 'wonderfull', 'wonderfully', 'wondering', 'wonderland', 'wonders', 'wondrous', 'wong', 'wonky', 'wont', 'woo', 'wood', 'woodbury', 'wooded', 'wooden', 'woodhouse', 'woodhull', 'woodlock', 'woods', 'woody', 'woohoo', 'wool', 'word', 'wordless', 'words', 'wore', 'work', 'workable', 'workaholic', 'workaholics', 'worked', 'worker', 'workers', 'working', 'workings', 'workout', 'workplace', 'workroom', 'works', 'world', 'worldly', 'worlds', 'worldview', 'worldwide', 'worm', 'wormhole', 'worms', 'worn', 'worried', 'worries', 'worrisome', 'worry', 'worrying', 'worse', 'worship', 'worshiping', 'worst', 'worth', 'worthington', 'worthless', 'worthwhile', 'worthy', 'woudl', 'would', 'woulda', 'wouldn', 'wouldnt', 'wound', 'wounded', 'wounds', 'wove', 'woven', 'wow', 'wowed', 'wracking', 'wraith', 'wraiths', 'wrangling', 'wrap', 'wraparound', 'wrapped', 'wrapping', 'wraps', 'wrath', 'wreak', 'wreaking', 'wreaks', 'wreck', 'wreckage', 'wrecked', 'wrecking', 'wrecks', 'wrench', 'wrenching', 'wrestle', 'wrestling', 'wretched', 'wright', 'wringer', 'wrinkle', 'wrinkled', 'wrinkles', 'wrist', 'write', 'writer', 'writers', 'writes', 'writing', 'writings', 'written', 'writters', 'writting', 'wrong', 'wronged', 'wrongful', 'wrongfully', 'wrongly', 'wrongs', 'wrote', 'wrought', 'wrung', 'wry', 'ws', 'wtf', 'wth', 'wu', 'wuornos', 'wuss', 'ww', 'ww2', 'wwe', 'wwi', 'wwii', 'wwiii', 'www', 'wy', 'wydra', 'wyle', 'wylie', 'wyman', 'wymark', 'wynn', 'wynona', 'wyoming', 'xan', 'xavier', 'xbox', 'xena', 'xfiles', 'xfinity', 'xmas', 'xo', 'xplr', 'xu', 'xy', 'ya', 'yacht', 'yada', 'yakusho', 'yakuza', 'yang', 'yank', 'yanked', 'yankee', 'yanks', 'yara', 'yard', 'yards', 'yarn', 'yarns', 'yates', 'yawn', 'yawns', 'yay', 'yayaying', 'yazpik', 'ye', 'yea', 'yeah', 'year', 'yearly', 'yearning', 'yearns', 'years', 'yeh', 'yelchin', 'yell', 'yelled', 'yelling', 'yellow', 'yells', 'yep', 'yes', 'yesterday', 'yesteryear', 'yet', 'ygg', 'yield', 'yikes', 'yin', 'yo', 'yoba', 'yoga', 'yogurt', 'yolanda', 'york', 'yorker', 'yorkers', 'yorkshire', 'yost', 'you', 'young', 'younger', 'youngest', 'youngsters', 'your', 'yours', 'yourself', 'yourselves', 'youth', 'youthful', 'youtube', 'yr', 'yrs', 'yuck', 'yucky', 'yuk', 'yukon', 'yule', 'yum', 'yuma', 'yummy', 'yup', 'yuppie', 'yuppies', 'yuri', 'yvonne', 'zac', 'zach', 'zachary', 'zack', 'zaffis', 'zagorsky', 'zahn', 'zak', 'zane', 'zany', 'zayas', 'zea', 'zealand', 'zealot', 'zeitgeist', 'zeke', 'zeljko', 'zeman', 'zen', 'zero', 'zest', 'zeta', 'zhao', 'zigzags', 'zillion', 'zing', 'zingers', 'zip', 'ziva', 'zo', 'zodiac', 'zoe', 'zoey', 'zombie', 'zombieland', 'zombies', 'zombified', 'zone', 'zoned', 'zones', 'zoo', 'zooey', 'zoom']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()[-1111:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Removing Misspells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-c0256b8e8d9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# This comes from CPMP script in the Quora questions similarity challenge. \n",
    "import re\n",
    "from collections import Counter\n",
    "import gensim\n",
    "import heapq\n",
    "from operator import itemgetter\n",
    "from multiprocessing import Pool\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin', \n",
    "                                                        binary=True)\n",
    "words = model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(train.question_text)\n",
    "\n",
    "top_90k_words = dict(heapq.nlargest(90000, vocab.items(), key=itemgetter(1)))\n",
    "\n",
    "pool = Pool(4)\n",
    "corrected_words = pool.map(correction,list(top_90k_words.keys()))\n",
    "\n",
    "for word,corrected_word in zip(top_90k_words,corrected_words):\n",
    "    if word!=corrected_word:\n",
    "        print(word,\":\",corrected_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/3a/32a1edf4f335eba0873021a7ddb3230f05dedd2b5450960118b402ca0771/gensim-3.8.0-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.7MB)\n",
      "\u001b[K     |████████████████████████████████| 24.7MB 7.7MB/s eta 0:00:01     |█████████████████▏              | 13.3MB 3.1MB/s eta 0:00:04     |█████████████████████           | 16.2MB 3.1MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting smart-open>=1.7.0 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/c0/25d19badc495428dec6a4bf7782de617ee0246a9211af75b302a2681dea7/smart_open-1.8.4.tar.gz (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 7.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from gensim) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: boto>=2.32 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim) (2.20.1)\n",
      "Collecting boto3 (from smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/80/193be5d386275cef24fc1c8e6127be557e0db978b4cf96dce57727236e6f/boto3-1.9.208-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 8.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.7.0->gensim) (2018.11.29)\n",
      "Collecting s3transfer<0.3.0,>=0.2.0 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 7.0MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting botocore<1.13.0,>=1.12.208 (from boto3->smart-open>=1.7.0->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/49/02ca4145a6ea491028093567077ad90a1e5cec86d3d7b0d01b58b643567f/botocore-1.12.208-py2.py3-none-any.whl (5.7MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7MB 9.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.7.0->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.208->boto3->smart-open>=1.7.0->gensim) (2.7.5)\n",
      "Requirement already satisfied: docutils<0.15,>=0.10 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.208->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ZYe/Library/Caches/pip/wheels/5f/ea/fb/5b1a947b369724063b2617011f1540c44eb00e28c3d2ca8692\n",
      "Successfully built smart-open\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.208 botocore-1.12.208 gensim-3.8.0 jmespath-0.9.4 s3transfer-0.2.1 smart-open-1.8.4\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Capstone)",
   "language": "python",
   "name": "pycharm-a015485f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}