{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from func import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
    "from spacy.tokens import Doc\n",
    "import numpy\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(data = 'musical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10261, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['reviewText']\n",
    "y = df['overall']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_bottom_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n], words_freq[-n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words, infreq_words = get_top_bottom_n_words(df['reviewText'], n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1103 16:01:55.001188 4563981760 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I1103 16:01:55.039225 4563981760 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'sentencizer', 'custom_model_wrapper']\n",
      "sentence run \n"
     ]
    }
   ],
   "source": [
    "def remove_tokens_on_match(doc):\n",
    "    indexes = []\n",
    "    for index, token in enumerate(doc):\n",
    "        if token.is_stop:\n",
    "            indexes.append(index)\n",
    "        if (token.pos_  in ('PUNCT', 'NUM', 'SYM')):\n",
    "            indexes.append(index)\n",
    "    np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
    "\n",
    "    np_array = numpy.delete(np_array, indexes, axis = 0)\n",
    "    doc2 = Doc(doc.vocab, words=[t.lemma_ for i, t in enumerate(doc) if i not in indexes])\n",
    "    doc2.from_array([LOWER, POS, ENT_TYPE, IS_ALPHA], np_array)\n",
    "    return doc2\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "nlp.add_pipe(remove_tokens_on_match, name=\"custom_model_wrapper\", last=True)\n",
    "\n",
    "print(nlp.pipe_names)  # ['tagger', 'parser', 'ner', 'print_info']\n",
    "doc = nlp(\"This is a 4 sentences running.\")\n",
    "# for token in doc:\n",
    "#     print(token.lemma_)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing texts...\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n",
    "nlp.add_pipe(remove_tokens_on_match, name=\"custom_model_wrapper\", last=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "\n",
    "print(\"Parsing texts...\")\n",
    "train_docs = list(nlp.pipe(X_train))\n",
    "dev_docs = list(nlp.pipe(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labelled_sentences(docs, doc_labels):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for doc, y in zip(docs, doc_labels):\n",
    "        sentences.append(doc.text)\n",
    "        labels.append(y)\n",
    "    return sentences, numpy.asarray(labels, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, y_train = get_labelled_sentences(train_docs, y_train)\n",
    "test_data, y_test = get_labelled_sentences(dev_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_features(docs, n):\n",
    "\n",
    "    # Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=False, stop_words=\"english\", ngram_range=(1, n), max_df=0.99, min_df=0.01)\n",
    "\n",
    "    # Transform the training data: tfidf_train \n",
    "    tfidf_df = tfidf_vectorizer.fit_transform(docs)\n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = get_features(train_data, 2)\n",
    "test_feature = get_features(test_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('model', SVC())\n",
    "])\n",
    "\n",
    "# single categorical value of 'model' parameter is \n",
    "# sets the model class\n",
    "# We will get ConvergenceWarnings because the problem is not well-conditioned.\n",
    "# But that's fine, this is just an example.\n",
    "linsvc_search = {\n",
    "    'model': [LinearSVC(max_iter=1000)],\n",
    "    'model__C': (1e-6, 1e+6, 'log-uniform'),\n",
    "}\n",
    "\n",
    "# explicit dimension classes can be specified like this\n",
    "svc_search = {\n",
    "    'model': Categorical([SVC()]),\n",
    "    'model__C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
    "    'model__gamma': Real(1e-6, 1e+1, prior='log-uniform'),\n",
    "    'model__degree': Integer(1,8),\n",
    "    'model__kernel': Categorical(['linear', 'poly', 'rbf']),\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    pipe,\n",
    "    [(svc_search, 20), (linsvc_search, 16)], # (parameter space, # of evaluations)\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt.fit(train_feature, y_train)\n",
    "\n",
    "print(\"val. score: %s\" % opt.best_score_)\n",
    "print(\"test score: %s\" % opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-optimize\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14.0 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from scikit-optimize) (1.1.0)\n",
      "Requirement already satisfied: numpy in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from scikit-optimize) (1.16.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /Users/ZYe/anaconda3/lib/python3.7/site-packages (from scikit-optimize) (0.20.1)\n",
      "Installing collected packages: scikit-optimize\n",
      "Successfully installed scikit-optimize-0.5.2\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_normalization(doc):\n",
    "#     nlp = spacy.load('en_core_web_sm')\n",
    "#     punctuations = string.punctuation\n",
    "#     doc = nlp(doc, disable=['parser', 'ner'])\n",
    "#     tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "# #     tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "#     tokens = ' '.join(tokens)\n",
    "#     return tokens\n",
    "# normalize_corpus = np.vectorize(text_normalization)\n",
    "# df['reviewText_norm'] = normalize_corpus(df['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def doc_normalization(doc):\n",
    "#     # lower case and remove special characters\\whitespaces\n",
    "#     doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "#     return doc\n",
    "# normalize_corpus2 = np.vectorize(doc_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText_norm1'] = normalize_corpus2(df['reviewText_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['reviewText_norm1']\n",
    "y = df['overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,4))\n",
    "sns.barplot(x = y.unique(), y=y.value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.18954969,   9.71784293,   2.61209824, ...,  -7.36065031,\n",
       "          9.63490509,  17.44544087],\n",
       "       [  4.08139378,  16.95146386,   6.28309358, ...,  -1.01094267,\n",
       "         16.25371694, -15.86553132],\n",
       "       [ -2.22079883,  -9.53135864, -10.78474115, ...,   2.30957298,\n",
       "         -8.37513194,  -0.55010412],\n",
       "       [ -3.38914669,  -8.88140091,   0.93966102, ...,   2.94122731,\n",
       "         -8.56182499,  -0.47323822],\n",
       "       [ -2.66099795,  -8.25654723,   0.94988831, ...,   3.12079269,\n",
       "         -8.9516651 ,  -0.55656721]])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()\n",
    "coefs_with_fns = sorted(zip(lr_classifier.coef_[1], feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns_1 = sorted(zip(clf.coef_[0], feature_names))\n",
    "    coefs_with_fns_2 = sorted(zip(clf.coef_[1], feature_names))\n",
    "    coefs_with_fns_3 = sorted(zip(clf.coef_[2], feature_names))\n",
    "    coefs_with_fns_4 = sorted(zip(clf.coef_[3], feature_names))\n",
    "    coefs_with_fns_5 = sorted(zip(clf.coef_[4], feature_names))\n",
    "    topClass1 = coefs_with_fns_1[:N]\n",
    "    topClass2 = coefs_with_fns_2[:N]\n",
    "    topClass3 = coefs_with_fns_3[:N]\n",
    "    topClass4 = coefs_with_fns_4[:N]\n",
    "    topClass5 = coefs_with_fns_5[:N]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "    print(\"Class 3 best: \")\n",
    "    for feat in topClass3:\n",
    "        print(feat)\n",
    "    print(\"Class 4 best: \")\n",
    "    for feat in topClass4:\n",
    "        print(feat)\n",
    "    print(\"Class 5 best: \")\n",
    "    for feat in topClass5:\n",
    "        print(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 best: \n",
      "(-29.204402286374467, 'option')\n",
      "(-27.840546778395055, 'guy')\n",
      "(-27.797088541698155, 'suppose')\n",
      "(-25.5520944699803, 'plan')\n",
      "(-25.43985038464847, 'hit')\n",
      "(-24.527600727074635, 'peg')\n",
      "(-23.776952564981293, 'favorite')\n",
      "(-23.398133943576514, 'probably')\n",
      "(-22.379679906569336, 'expect')\n",
      "(-21.19470398846132, 'particular')\n",
      "(-20.816322131522433, 'string sound')\n",
      "(-20.569628916895166, 'edge')\n",
      "(-20.229851931322198, 'recently')\n",
      "(-20.177408097359283, 'pitch')\n",
      "(-19.962573814628257, 'turn')\n",
      "Class 2 best: \n",
      "(-96.03952673383505, 'red')\n",
      "(-83.21681824885088, 'body')\n",
      "(-74.0987313020265, 'arrive')\n",
      "(-71.88763685405871, 'center')\n",
      "(-70.8305296309087, 'little bit')\n",
      "(-67.1023761653878, 'vocal')\n",
      "(-66.2774242663929, 'exactly')\n",
      "(-66.24714624804609, 'mix')\n",
      "(-65.46152053523342, 'just fine')\n",
      "(-63.31121622726044, 'mess')\n",
      "(-63.16498836286109, 'mandolin')\n",
      "(-62.73790666607934, 'string use')\n",
      "(-62.15661042928743, 'high end')\n",
      "(-59.314347013333574, 'course')\n",
      "(-58.663426200857515, 'really good')\n",
      "Class 3 best: \n",
      "(-12.308096815797152, 'extremely')\n",
      "(-11.561590505734742, 'return')\n",
      "(-11.433079385144815, 'sound like')\n",
      "(-11.374132574497274, 'power supply')\n",
      "(-10.784741152624237, 'ac')\n",
      "(-10.713644296503345, 'hope')\n",
      "(-10.236769135912349, 'software')\n",
      "(-10.11635440670603, 'channel')\n",
      "(-9.745319183585433, 'tube amp')\n",
      "(-9.531358641800248, 'absolutely')\n",
      "(-9.27934862457965, 'stop')\n",
      "(-8.987690212496362, 'sit')\n",
      "(-8.792066413638912, 'fall')\n",
      "(-8.731618829890758, 'believe')\n",
      "(-8.701301635716941, 'flexible')\n",
      "Class 4 best: \n",
      "(-13.532611181318078, 'power supply')\n",
      "(-11.953660227327223, 'sound like')\n",
      "(-11.88311053900556, 'hope')\n",
      "(-11.47015247384775, 'extremely')\n",
      "(-11.46269185450191, 'return')\n",
      "(-11.316903493718435, 'software')\n",
      "(-9.849557425426164, 'tube amp')\n",
      "(-9.700607558365167, 'channel')\n",
      "(-9.630309006173364, 'flexible')\n",
      "(-9.452421983384502, 'fail')\n",
      "(-9.32634559522618, 'difficult')\n",
      "(-8.993532239089085, 'fall')\n",
      "(-8.944799491967357, 'equipment')\n",
      "(-8.881400907774212, 'absolutely')\n",
      "(-8.87863849520921, 'idea')\n",
      "Class 5 best: \n",
      "(-14.31244125632038, 'power supply')\n",
      "(-12.680417626178178, 'hope')\n",
      "(-11.647997038818618, 'sound like')\n",
      "(-11.632795175291584, 'software')\n",
      "(-11.478007700861568, 'return')\n",
      "(-11.415692783469343, 'les paul')\n",
      "(-10.946336534319899, 'extremely')\n",
      "(-10.31943689474685, 'difficult')\n",
      "(-9.624033994140488, 'tube amp')\n",
      "(-9.287574622795686, 'idea')\n",
      "(-9.106491792121911, 'fail')\n",
      "(-9.062499453344875, 'fall')\n",
      "(-9.008986221343239, 'channel')\n",
      "(-8.951665104336904, 'year ago')\n",
      "(-8.884058037821685, 'look like')\n"
     ]
    }
   ],
   "source": [
    "printNMostInformative(count_vectorizer, lr_classifier, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the Most Frequent words for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO_text = [text for text in X_train[X_train['Conference'] == 'INFOCOM']['Title']]\n",
    "IS_text = [text for text in train[train['Conference'] == 'ISCAS']['Title']]\n",
    "INFO_clean = cleanup_text(INFO_text)\n",
    "INFO_clean = ' '.join(INFO_clean).split()\n",
    "IS_clean = cleanup_text(IS_text)\n",
    "IS_clean = ' '.join(IS_clean).split()\n",
    "INFO_counts = Counter(INFO_clean)\n",
    "IS_counts = Counter(IS_clean)\n",
    "INFO_common_words = [word[0] for word in INFO_counts.most_common(20)]\n",
    "INFO_common_counts = [word[1] for word in INFO_counts.most_common(20)]\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "sns.barplot(x=INFO_common_words, y=INFO_common_counts)\n",
    "plt.title('Most Common Words used in the research papers for conference INFOCOM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cond(tfidf_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.796461273932955"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cond(tfidf_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'absolutely', 'ac', 'accurate', 'acoustic', 'acoustic electric', 'acoustic guitar', 'action', 'actually', 'adapter', 'add', 'adjust', 'adjustable', 'adjustment', 'advertise', 'ago', 'allow', 'amazing', 'amazon', 'amp', 'angle', 'apart', 'appear', 'arm', 'arrive', 'ask', 'attach', 'audio', 'available', 'away', 'awesome', 'bad', 'bag', 'ball', 'band', 'bar', 'base', 'basic', 'basically', 'bass', 'battery', 'beat', 'beginner', 'behringer', 'believe', 'bend', 'best', 'better', 'big', 'bit', 'black', 'blue', 'board', 'body', 'boost', 'boss', 'box', 'brand', 'break', 'bridge', 'bright', 'bring', 'buck', 'budget', 'build', 'build quality', 'button', 'buy', 'buzz', 'cable', 'capo', 'care', 'carry', 'case', 'cause', 'center', 'certainly', 'chain', 'change', 'change string', 'channel', 'cheap', 'check', 'choice', 'choose', 'chord', 'clamp', 'classic', 'clean', 'clear', 'clip', 'close', 'cloth', 'color', 'come', 'comfortable', 'compact', 'company', 'compare', 'complain', 'complaint', 'completely', 'computer', 'connect', 'connection', 'connector', 'consider', 'construction', 'control', 'cool', 'cord', 'cost', 'couple', 'course', 'cover', 'cut', 'daddario', 'damage', 'day', 'deal', 'decent', 'decide', 'definitely', 'delay', 'deliver', 'depend', 'design', 'device', 'dial', 'difference', 'different', 'difficult', 'digital', 'display', 'distortion', 'dollar', 'double', 'drive', 'drop', 'drum', 'dunlop', 'durable', 'duty', 'ear', 'easily', 'easy', 'easy use', 'edge', 'effect', 'electric', 'electric guitar', 'elixir', 'end', 'enjoy', 'epiphone', 'eq', 'equipment', 'ernie', 'especially', 'exactly', 'excellent', 'expect', 'expensive', 'experience', 'extra', 'extremely', 'fact', 'fail', 'fairly', 'fall', 'fan', 'fantastic', 'far', 'fast', 'favorite', 'feature', 'feel', 'feel like', 'fender', 'figure', 'filter', 'finally', 'fine', 'finger', 'finish', 'fit', 'fix', 'flat', 'flexible', 'floor', 'fold', 'foot', 'free', 'fret', 'friend', 'fun', 'function', 'gain', 'gauge', 'gear', 'gibson', 'gig', 'gig bag', 'glad', 'good', 'good price', 'good quality', 'good sound', 'great', 'great price', 'great product', 'great sound', 'grip', 'guess', 'guitar', 'guitar case', 'guitar pick', 'guitar player', 'guitar sound', 'guitar stand', 'guitar strap', 'guitar string', 'guitarist', 'guy', 'half', 'hand', 'handle', 'handy', 'hang', 'happen', 'happy', 'hard', 'head', 'headphone', 'headstock', 'hear', 'heavy', 'help', 'high', 'high end', 'high quality', 'highly', 'highly recommend', 'hit', 'hold', 'holder', 'hole', 'home', 'hook', 'hope', 'hour', 'house', 'huge', 'idea', 'impressed', 'improve', 'inch', 'include', 'inexpensive', 'input', 'inside', 'instal', 'install', 'instead', 'instrument', 'interface', 'issue', 'item', 'jack', 'jam', 'jazz', 'job', 'just', 'just fine', 'just like', 'just right', 'key', 'keyboard', 'kind', 'knob', 'know', 'large', 'later', 'lead', 'learn', 'leather', 'leave', 'length', 'les', 'les paul', 'let', 'level', 'life', 'light', 'like', 'likely', 'line', 'little', 'little bit', 'live', 'local', 'lock', 'long', 'long time', 'longer', 'look', 'look good', 'look great', 'look like', 'loop', 'loose', 'lose', 'lot', 'loud', 'love', 'low', 'main', 'make', 'make easy', 'make sure', 'mandolin', 'martin', 'match', 'material', 'matter', 'maybe', 'mean', 'medium', 'mention', 'mess', 'metal', 'mic', 'mic stand', 'microphone', 'mid', 'mind', 'mini', 'minute', 'miss', 'mix', 'mixer', 'mm', 'model', 'money', 'month', 'mount', 'music', 'musician', 'mxr', 'near', 'neck', 'need', 'new', 'nice', 'nicely', 'noise', 'non', 'normal', 'note', 'notice', 'nut', 'nylon', 'offer', 'ok', 'old', 'open', 'opinion', 'option', 'order', 'original', 'output', 'overall', 'overdrive', 'pack', 'package', 'pad', 'padding', 'particular', 'past', 'patch', 'paul', 'pay', 'pedal', 'pedal board', 'peg', 'people', 'perfect', 'perfectly', 'perform', 'performance', 'pick', 'pickup', 'picture', 'piece', 'pin', 'pitch', 'place', 'plan', 'planet', 'planet waves', 'plastic', 'play', 'play guitar', 'player', 'playing', 'pleased', 'plenty', 'plug', 'plus', 'pocket', 'point', 'pop', 'portable', 'position', 'power', 'power supply', 'practice', 'prefer', 'pretty', 'pretty good', 'price', 'pro', 'probably', 'problem', 'produce', 'product', 'professional', 'properly', 'protect', 'provide', 'pull', 'purchase', 'purpose', 'push', 'quality', 'quick', 'quickly', 'quiet', 'quite', 'range', 'rate', 'read', 'real', 'really', 'really good', 'really like', 'really nice', 'reason', 'receive', 'recently', 'recommend', 'record', 'recording', 'red', 'regular', 'remove', 'replace', 'replacement', 'require', 'rest', 'result', 'return', 'reverb', 'review', 'reviewer', 'rich', 'right', 'rock', 'room', 'rubber', 'run', 'save', 'say', 'screw', 'second', 'secure', 'sell', 'send', 'set', 'setting', 'setup', 'shape', 'ship', 'shipping', 'shop', 'short', 'shure', 'signal', 'similar', 'simple', 'simply', 'single', 'sit', 'size', 'slide', 'slightly', 'slip', 'small', 'smooth', 'snark', 'soft', 'software', 'solid', 'son', 'song', 'soon', 'sort', 'sound', 'sound good', 'sound great', 'sound like', 'sound quality', 'sounding', 'space', 'speaker', 'spend', 'spot', 'spring', 'stable', 'stage', 'stand', 'standard', 'star', 'start', 'state', 'stay', 'stay tune', 'steel', 'stick', 'stiff', 'stock', 'stop', 'store', 'straight', 'strap', 'strap lock', 'strat', 'string', 'string guitar', 'string sound', 'string use', 'strong', 'studio', 'stuff', 'sturdy', 'style', 'suggest', 'super', 'supply', 'support', 'suppose', 'sure', 'surface', 'surprised', 'sustain', 'sweet', 'switch', 'tell', 'tend', 'tension', 'test', 'thank', 'thickness', 'thing', 'think', 'tight', 'time', 'tip', 'tone', 'tool', 'totally', 'touch', 'track', 'travel', 'true', 'try', 'tube', 'tube amp', 'tune', 'tuner', 'tuning', 'turn', 'type', 'uke', 'ukulele', 'unit', 'unless', 'update', 'upgrade', 'usb', 'use', 'use guitar', 'useful', 'user', 'usually', 'value', 'variety', 'various', 'versatile', 'version', 'video', 'vintage', 'vocal', 'voice', 'volume', 'wall', 'want', 'warm', 'wave', 'waves', 'way', 'wear', 'week', 'weight', 'wide', 'wire', 'wish', 'wonderful', 'wood', 'work', 'work fine', 'work great', 'work just', 'work perfectly', 'worry', 'worth', 'wow', 'write', 'wrong', 'xlr', 'year', 'year ago', 'yes']\n"
     ]
    }
   ],
   "source": [
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words = \"english\", ngram_range=(1, 2), min_df=.01, max_df=.99)\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using only the 'tex t' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "633"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6874, 633)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_train.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(count_vectorizer.get_feature_names()[:700])\n",
    "\n",
    "# X_train1 = X_train.apply(lambda sentence: \n",
    "#                          ' '.join([word for word in sentence.split() if word.lower() not in stop_words])) #\n",
    "\n",
    "# X_train\n",
    "\n",
    "# X_train1\n",
    "\n",
    "# X_test1 = X_test.apply(lambda sentence: \n",
    "#                        ' '.join([word for word in sentence.split() if word.lower() not in stop_words])) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize a CountVectorizer object: count_vectorizer\n",
    "# count_vectorizer1 = CountVectorizer(stop_words = \"english\")\n",
    "\n",
    "# # Transform the training data using only the 'text' column values: count_train \n",
    "# count_train = count_vectorizer1.fit_transform(X_train1)\n",
    "\n",
    "# # Transform the test data using only the 'tex t' column values: count_test \n",
    "# count_test = count_vectorizer1.transform(X_test1)\n",
    "\n",
    "# # Print the first 10 features of the count_vectorizer\n",
    "# print(count_vectorizer1.get_feature_names()[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZYe/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/ZYe/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "            'prec': 'precision'}\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: lr_classifier\n",
    "lr_classifier = LogisticRegression(C=1e5, solver='newton-cg', multi_class='multinomial') \n",
    "                    #, class_weight = weights)\n",
    "\n",
    "scores = cross_validate(lr_classifier, count_train, y_train, cv=3,\n",
    "                         scoring=['accuracy', 'f1_weighted'],\n",
    "                         return_train_score=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.33208585, 0.32400393, 0.24904299]),\n",
       " 'score_time': array([0.00259519, 0.00169301, 0.002002  ]),\n",
       " 'test_accuracy': array([0.66768426, 0.67568747, 0.67641921]),\n",
       " 'train_accuracy': array([0.6780179 , 0.67706742, 0.67822862]),\n",
       " 'test_f1_weighted': array([0.54921086, 0.55338102, 0.55846713]),\n",
       " 'train_f1_weighted': array([0.55869991, 0.55555631, 0.56038014])}"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_df=0.99, min_df=0.01)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier: lr_classifier\n",
    "lr_classifier = LogisticRegression(C=1e5, solver='newton-cg', multi_class='multinomial') \n",
    "                    #, class_weight = weights)\n",
    "\n",
    "scores = cross_validate(lr_classifier, tfidf_train, y_train, cv=3,\n",
    "                         scoring=['accuracy', 'f1_weighted'],\n",
    "                         return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([8.70167112, 9.3029089 , 7.52045417]),\n",
       " 'score_time': array([0.00242686, 0.00211096, 0.00213671]),\n",
       " 'test_accuracy': array([0.60052333, 0.6001746 , 0.60305677]),\n",
       " 'train_accuracy': array([0.7987339 , 0.8020947 , 0.79057592]),\n",
       " 'test_f1_weighted': array([0.59032733, 0.58622763, 0.59111767]),\n",
       " 'train_f1_weighted': array([0.7828489 , 0.7856703 , 0.77327812])}"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6194272217301446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ZYe/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:203: ConvergenceWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  \"number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a Multinomial Naive Bayes classifier: lr_classifier\n",
    "lr_classifier = LogisticRegression(C=1e5, solver='newton-cg', multi_class='multinomial')\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "lr_classifier.fit(count_train,y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = lr_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "# from sklearn.metrics import multilabel_confusion_matrix\n",
    "# cm = multilabel_confusion_matrix(y_test, pred, labels=['1Star',\"2Star\", '3Star', '4Star', '5Star'])\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we will use logistic regression model trained\n",
    "using Keras in spaCy. spaCy splits the document into sentences, and each\n",
    "sentence is classified using the LSTM. The scores for the sentences are then\n",
    "aggregated to give the document score. This kind of hierarchical model is quite\n",
    "difficult in \"pure\" Keras or Tensorflow, but it's very effective. The Keras\n",
    "example on this dataset performs quite poorly, because it cuts off the documents\n",
    "so that they're a fixed size. This hurts review accuracy a lot, because people\n",
    "often summarise their rating in the final sentence\n",
    "Prerequisites:\n",
    "spacy download en_vectors_web_lg\n",
    "pip install keras==2.0.9\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "\n",
    "import plac\n",
    "import random\n",
    "import pathlib\n",
    "import cytoolz\n",
    "import numpy\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "import thinc.extra.datasets\n",
    "from spacy.compat import pickle\n",
    "import spacy\n",
    "\n",
    "\n",
    "class SentimentAnalyser(object):\n",
    "    @classmethod\n",
    "    def load(cls, path, nlp, max_length=100):\n",
    "        with (path / \"config.json\").open() as file_:\n",
    "            model = model_from_json(file_.read())\n",
    "        with (path / \"model\").open(\"rb\") as file_:\n",
    "            lstm_weights = pickle.load(file_)\n",
    "        embeddings = get_embeddings(nlp.vocab)\n",
    "        model.set_weights([embeddings] + lstm_weights)\n",
    "        return cls(model, max_length=max_length)\n",
    "\n",
    "    def __init__(self, model, max_length=100):\n",
    "        self._model = model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        X = get_features([doc], self.max_length)\n",
    "        y = self._model.predict(X)\n",
    "        self.set_sentiment(doc, y)\n",
    "\n",
    "    def pipe(self, docs, batch_size=1000):\n",
    "        for minibatch in cytoolz.partition_all(batch_size, docs):\n",
    "            minibatch = list(minibatch)\n",
    "            sentences = []\n",
    "            for doc in minibatch:\n",
    "                sentences.extend(doc.sents)\n",
    "            Xs = get_features(sentences, self.max_length)\n",
    "            ys = self._model.predict(Xs)\n",
    "            for sent, label in zip(sentences, ys):\n",
    "                sent.doc.sentiment += label - 0.5\n",
    "            for doc in minibatch:\n",
    "                yield doc\n",
    "\n",
    "    def set_sentiment(self, doc, y):\n",
    "        doc.sentiment = float(y[0])\n",
    "        # Sentiment has a native slot for a single float.\n",
    "        # For arbitrary data storage, there's:\n",
    "        # doc.user_data['my_data'] = y\n",
    "\n",
    "\n",
    "def get_labelled_sentences(docs, doc_labels):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for doc, y in zip(docs, doc_labels):\n",
    "        for sent in doc.sents:\n",
    "            sentences.append(sent)\n",
    "            labels.append(y)\n",
    "    return sentences, numpy.asarray(labels, dtype=\"int32\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_features(docs, n):\n",
    "\n",
    "    # Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, n), max_df=0.99, min_df=0.01)\n",
    "\n",
    "    # Transform the training data: tfidf_train \n",
    "    tfidf_df = tfidf_vectorizer.fit_transform(docs)\n",
    "    return tfidf_df\n",
    "#     docs = list(docs)\n",
    "#     Xs = numpy.zeros((len(docs), max_length), dtype=\"int32\")\n",
    "#     for i, doc in enumerate(docs):\n",
    "#         j = 0\n",
    "#         for token in doc:\n",
    "#             vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "#             if vector_id >= 0:\n",
    "#                 Xs[i, j] = vector_id\n",
    "#             else:\n",
    "#                 Xs[i, j] = 0\n",
    "#             j += 1\n",
    "#             if j >= max_length:\n",
    "#                 break\n",
    "#     return Xs\n",
    "\n",
    "def remove_tokens_on_match(doc):\n",
    "    indexes = []\n",
    "    for index, token in enumerate(doc):\n",
    "        if token.is_stop:\n",
    "            indexes.append(index)\n",
    "        if (token.pos_  in ('PUNCT', 'NUM', 'SYM')):\n",
    "            indexes.append(index)\n",
    "    np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
    "\n",
    "    np_array = numpy.delete(np_array, indexes, axis = 0)\n",
    "    doc2 = Doc(doc.vocab, words=[t.lemma_ for i, t in enumerate(doc) if i not in indexes])\n",
    "    doc2.from_array([LOWER, POS, ENT_TYPE, IS_ALPHA], np_array)\n",
    "    return doc2\n",
    "\n",
    "def train(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    dev_texts,\n",
    "    dev_labels,\n",
    "    lstm_shape,\n",
    "    lstm_settings,\n",
    "    lstm_optimizer,\n",
    "    batch_size=100,\n",
    "    nb_epoch=5,\n",
    "    by_sentence=True,\n",
    "):\n",
    "\n",
    "    print(\"Loading spaCy\")\n",
    "    \n",
    "    \n",
    "#     nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    \n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n",
    "    nlp.add_pipe(remove_tokens_on_match, name=\"custom_model_wrapper\", last=True)\n",
    "#     nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "#     embeddings = get_embeddings(nlp.vocab)\n",
    "    model = compile_logisticRegression(c)\n",
    "\n",
    "    print(\"Parsing texts...\")\n",
    "    train_docs = list(nlp.pipe(train_texts))\n",
    "    dev_docs = list(nlp.pipe(dev_texts))\n",
    "#     if by_sentence:\n",
    "#         train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
    "#         dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
    "\n",
    "    train_X = get_features(train_docs, lstm_shape[\"max_length\"])\n",
    "    dev_X = get_features(dev_docs, lstm_shape[\"max_length\"])\n",
    "    model.fit(\n",
    "        train_X,\n",
    "        train_labels,\n",
    "        validation_data=(dev_X, dev_labels),\n",
    "        epochs=nb_epoch,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_logisticRegression(c=1e5):\n",
    "    model = LogisticRegression(C=c, solver='newton-cg', multi_class='multinomial') \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embeddings(vocab):\n",
    "    return vocab.vectors.data\n",
    "\n",
    "\n",
    "def evaluate(model_dir, texts, labels, max_length=100):\n",
    "    nlp = spacy.load(\"en_vectors_web_lg\")\n",
    "    nlp.add_pipe(nlp.create_pipe(\"sentencizer\"))\n",
    "    nlp.add_pipe(SentimentAnalyser.load(model_dir, nlp, max_length=max_length))\n",
    "\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    for doc in nlp.pipe(texts, batch_size=1000):\n",
    "        correct += bool(doc.sentiment >= 0.5) == bool(labels[i])\n",
    "        i += 1\n",
    "    return float(correct) / i\n",
    "\n",
    "\n",
    "def read_data(data_dir, limit=0):\n",
    "    examples = []\n",
    "    for subdir, label in ((\"pos\", 1), (\"neg\", 0)):\n",
    "        for filename in (data_dir / subdir).iterdir():\n",
    "            with filename.open() as file_:\n",
    "                text = file_.read()\n",
    "            examples.append((text, label))\n",
    "    random.shuffle(examples)\n",
    "    if limit >= 1:\n",
    "        examples = examples[:limit]\n",
    "    return zip(*examples)  # Unzips into two lists\n",
    "\n",
    "\n",
    "@plac.annotations(\n",
    "    train_dir=(\"Location of training file or directory\"),\n",
    "    dev_dir=(\"Location of development file or directory\"),\n",
    "    model_dir=(\"Location of output model directory\",),\n",
    "    is_runtime=(\"Demonstrate run-time usage\", \"flag\", \"r\", bool),\n",
    "    nr_hidden=(\"Number of hidden units\", \"option\", \"H\", int),\n",
    "    max_length=(\"Maximum sentence length\", \"option\", \"L\", int),\n",
    "    dropout=(\"Dropout\", \"option\", \"d\", float),\n",
    "    learn_rate=(\"Learn rate\", \"option\", \"e\", float),\n",
    "    nb_epoch=(\"Number of training epochs\", \"option\", \"i\", int),\n",
    "    batch_size=(\"Size of minibatches for training LSTM\", \"option\", \"b\", int),\n",
    "    nr_examples=(\"Limit to N examples\", \"option\", \"n\", int),\n",
    ")\n",
    "def main(\n",
    "    model_dir=None,\n",
    "    train_dir=None,\n",
    "    dev_dir=None,\n",
    "    is_runtime=False,\n",
    "    nr_hidden=64,\n",
    "    max_length=100,  # Shape\n",
    "    dropout=0.5,\n",
    "    learn_rate=0.001,  # General NN config\n",
    "    nb_epoch=5,\n",
    "    batch_size=256,\n",
    "    nr_examples=-1,\n",
    "):  # Training params\n",
    "    if model_dir is not None:\n",
    "        model_dir = pathlib.Path(model_dir)\n",
    "    if train_dir is None or dev_dir is None:\n",
    "        imdb_data = thinc.extra.datasets.imdb()\n",
    "    if is_runtime:\n",
    "        if dev_dir is None:\n",
    "            dev_texts, dev_labels = zip(*imdb_data[1])\n",
    "        else:\n",
    "            dev_texts, dev_labels = read_data(dev_dir)\n",
    "        acc = evaluate(model_dir, dev_texts, dev_labels, max_length=max_length)\n",
    "        print(acc)\n",
    "    else:\n",
    "        if train_dir is None:\n",
    "            train_texts, train_labels = zip(*imdb_data[0])\n",
    "        else:\n",
    "            print(\"Read data\")\n",
    "            train_texts, train_labels = read_data(train_dir, limit=nr_examples)\n",
    "        if dev_dir is None:\n",
    "            dev_texts, dev_labels = zip(*imdb_data[1])\n",
    "        else:\n",
    "            dev_texts, dev_labels = read_data(dev_dir, imdb_data, limit=nr_examples)\n",
    "        train_labels = numpy.asarray(train_labels, dtype=\"int32\")\n",
    "        dev_labels = numpy.asarray(dev_labels, dtype=\"int32\")\n",
    "        lstm = train(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            dev_texts,\n",
    "            dev_labels,\n",
    "            {\"nr_hidden\": nr_hidden, \"max_length\": max_length, \"nr_class\": 1},\n",
    "            {\"dropout\": dropout, \"lr\": learn_rate},\n",
    "            {},\n",
    "            nb_epoch=nb_epoch,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        weights = lstm.get_weights()\n",
    "        if model_dir is not None:\n",
    "            with (model_dir / \"model\").open(\"wb\") as file_:\n",
    "                pickle.dump(weights[1:], file_)\n",
    "            with (model_dir / \"config.json\").open(\"w\") as file_:\n",
    "                file_.write(lstm.to_json())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plac.call(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
